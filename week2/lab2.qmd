---
title: "Lab 2"
subtitle: "Rockfish survey gaps"
toc: true
editor: visual
format:
  html:
    other-links:
      - text: Download Source
        href: lab2.qmd
      - text: LA Storm Data
        href: data/la_storm.csv
      - text: RREAS Data
        href: data/rreas.csv
editor_options: 
  chunk_output_type: console
execute: 
  eval: false
---

## Learning objectives

In today's lab you will...

1.  **Fit** a linear model to real world data
2.  **Interpret** model coefficients
3.  **Simulate** data according to model assumptions and recover parameters

```{r}
#| label: setup

library(tidyverse)
theme_set(theme_bw(18))

```

## Storms and floods

We'll begin with an example of the relationship betwen precipitation and flooding during storms in LA. Save the *LA Storm Data* CSV to your `data/` folder and use it to fill in the following code chunk. Each row in the CSV file contains data from a storm in LA during the last fifteen years. `gage_ht` is the height of the Los Angeles River at the Sepulveda Dam (in feet) and `precip_wk_in` is the total precipitation measured during the previous week at the Hollywood Burbank Airport. You can find data like these through USGS and NCEI, respectively.

```{r}
#| label: q1

# Read the CSV file into a data frame
storm_data <- read_csv(here::here('week2', 'la_storm.csv'))

# Create a plot of the gage height and precipitation data
ggplot(storm_data, 
       aes(x = precip_wk_in, 
           y = gage_ht)) + 
  geom_point()

# x = predictor --> x predicts y 
```

**Q1: Which variables did you choose for the x- and y-axis? What does this imply about the *data generating process*?**

In Quarto documents, we can add statistical notation using LaTeX. We'll fill in the equation below to describe the model.

$$
\begin{align}
Gage &\sim Normal(\mu, \sigma) \\
\mu &= \beta_0 + \beta_1 * precip
\end{align}
$$

### Fit model

The `lm()` function fits a linear regression model to data. It has two important parameters: the formula defining the model, and the data frame containing the data.

```{r}
#| label: fit-storm-model

# First arg: model formula 
# second arg: data frame 
storm_mod <- lm(gage_ht ~ precip_wk_in, data = storm_data)

```

Once you've fit the model, use `summary()` to see the details, including the coefficients and $R^2$ value. $\hat \sigma$ is in there too, but doesn't print by default. Assign the result of `summary()` to a variable called `storm_summ` and try to find the standard deviation from there (hint: it's an element in that variable).

```{r}
#| label: summarize-storm-model

storm_sum <- summary(storm_mod)

# Finding sigma 
sigma <- storm_sum$sigma

# Finding coefficients
B_0 <- coef(storm_mod)[1]
# storm_sum$coefficients[1,1]

B_1 <- coef(storm_mod)[2]
# storm_sum$coefficients[2,1]

```

**Q2: What are the estimates for** $\hat \beta_0, \hat \beta_1, \hat \sigma$ **?**

sigma = 2.32 \\

B0 hat = 3.91

B1 hat = 2.17

### Interpret model

**Q3: In plain language, describe how you'd interpret** $\beta_0$ **and** $\beta_1$**. What do they represent?**

-   Y intercept is 3.91 - no rain, the river is already at a gage height of 3.91.

-   Slope 2.17 - With every addionally inch of rain, the gage height increased by 2.17.

Let's also interpret $R^2$, the measure of model fit. Recall $R^2$ describes the amount of variance *explained by the model*. In other words, how much variance did we "explain away" using the model's prediction line. In equations, it looks like this.

$$
\begin{align}
SST &= \sum_i (y_i - \bar y)^2 \\
SSE &= \sum_i (y_i - \hat y_i)^2 \\
R^2 &= \frac{SST-SSE}{SST}
\end{align}
$$

We can get $R^2$ directly from the model summary in R, but it's helpful for your learning to do it a couple times directly, too. Calculate $SST$, $SSE$, and $R^2$ arithmetically in the code chunk below. You can use the `coef()` function to get the model coefficient estimates, but don't use `residuals()`.

```{r}
#| label: q4

# Finding the variables manually 

y_bar <- mean(storm_data$gage_ht)

x <- storm_data$precip_wk_in # x data for all the points 
y_hat <- B_0 + B_1 * x

SST <- sum((storm_data$gage_ht - y_bar)^2)
SSE <- sum((storm_data$gage_ht - y_hat)^2)
# What fraction of the variance in gage hieght have we explained using precipation? 
R2 <- (SST - SSE) / SST 

# Precipation explains 55% variation in gage height 

```

**Q4: What's the difference between** $y$**,** $\bar y$**, and** $\hat y$**?**

-   y = observed/ individual gage heights

-   y bar = mean gage height

-   y hat = expected gage height

**Q5: What does the** $R^2$ **value tell you about the quality of the model (in plain language)?**

-   Removed about half of the variance

-   Error is reduced by 55%

-   Having precipitation reduced with error of our predictions by about 55%

-   The other 45% of the variation can be described by other variables

Recall that the purpose of a model is to *explain variance*.

**Q6: What's the estimated value of** $\hat \sigma$ **for the model? What's the value of** $\sigma$ **for the response variable itself? Say you needed to predict Los Angeles River gage height following a storm - what does the relationship between these two numbers say about your ability to make that prediction accurately?**

**WE SKIPPED THIS**

## Simulating data from a model\*\*\*\*

Simulating data from a model is a super power for your learning! This is a great tool for unambiguously testing your understanding. Here's the basic process:

1.  Read the model description in statistical notation
2.  Choose a set of parameters and predictor variable(s) for your simulation
3.  Use a random variable to generate a response, based on your parameters and predictor(s)
4.  Fit a model to your simulated data
5.  Check if your model's parameter estimates match what you chose

Mastering this process will give you a massive boost in learning new statistical tools!

Let's do it for linear regression.

### 1. Read the model description

Here's the description of a linear regression model in statistical notation.

$$
\begin{align}
y &\sim Normal(\mu, \sigma) \\
\mu &= \beta_0 + \beta_1 x
\end{align}
$$

### 2. Choose parameters, predictor(s)

We have three parameters in our model: $\beta_0$, $\beta_1$, and $\sigma$. You can choose any *valid* value for them.

```{r}
#| label: choose-params

beta_0 <- 2
beta_1 <- 4
sigma <- 0.5

```

Choose any *valid* value for x, too. In linear regression, the predictor can take any real value. Let's choose 100 numbers evenly spaced between 0 and 10. You could generate these randomly, too, but you don't *have* to.

```{r}
#| label: choose-pred

x <- seq(0,10, length.out = 100)
head(x)
```

### 3. Generate response

Check out our model description - it tells us what to do next. $\mu$ is calculated directly from $\beta_0$, $\beta_1$, and $x$, which we just chose. $y$ *is distributed as* a normal variable with mean $\mu$ (which we calculate) and standard deviation $\sigma$ (which we chose). *Is distributed as* is our clue we need to create a random variable.

```{r}
#| label: generate-response

mu <- beta_0 + beta_1 * x # mu = shifting as x shifts --> vector 

y <- rnorm(n = 100, mean = mu, sd = sigma)

length(x)

```

### 4. Fit the model

As we did above.

```{r}
#| label: fit-model

sim_dat <- tibble(x, y)
sim_mod <- lm(y ~ x, data = sim_dat)

```

### 5. Check your model

Do the model parameter estimates match our chosen simulation parameters?

```{r}
#| label: check-model

mod_summ <- summary(sim_mod)
beta_0_hat <- coef(sim_mod)[1]
beta_1_hat <- coef(sim_mod)[2]
sigma_hat <- mod_summ$sigma

# if you model was wrong, your beta_hats will be switched 
```

Try changing your chosen parameters for your model and see if you can recover them. As long as you keep your simulated data large enough (n \> 100) you should get good results.

As we start to use more complicated models, this procedure will become invaluable. The key thing about it is you can check your work: if you're consistently getting the wrong estimates for your parameters then you know you have an error in your understanding. That's what office hours are for!

## Rockfish survey gaps

**\*\*DO THIS TO REVIEW FOR THE MIDTERM\*\***

In the rest of this lab, you'll apply what you learned in the previous examples to fill gaps in a marine ecosystem survey.

The Rockfish Recruitment and Ecosystem Assessment Survey (RREAS) is a long-running survey of juvenile rockfish, their physical environment, and their predators along the US west coast. The survey is vitally important for understanding the population dynamics of economically and culturally important species, such as bocaccio. RREAS data inform fishery management decisions to protect threatened populations while supporting California's coastal economy. For more information about RREAS, see their [Story Map](https://arcg.is/1CmKau0).

![The US West Coast Groundfish Fishery harvested 1 million pounds of bocaccio in 2023, valued at \$650k. Source: NOAA Fisheries](images/clipboard-4097019708.png){width="400"}

The RREAS has run almost every spring for over 40 years. The "almost" qualifier is necessary because the COVID-19 pandemic greatly reduced survey effort in 2020. NOAA scientists used linear regression to fill the gap in their dataset with a proxy measurement: diets of common murres.

![A conceptual model of the dynamics governing murres, their prey, and the environment.](images/clipboard-1069203086.png){width="400"}

The common murre is a seabird that breeds in large numbers off the coast of California. In spring, pairs of murres raise one chick by bringing it juvenile fish, including rockfish, anchovy, and salmon. The composition of common murre diets on the Farallon Islands (\~30 miles west of San Francisco) has been recorded by biologists with Point Blue Conservation since the 1970s.

### Explore the data

Read the data from the rreas.csv file and create a scatter plot relating fish abundance indices to common murre diet.

```{r}
library(ggplot2)

rreas_murres <- read_csv("week2/rreas.csv")

colnames(rreas_murres)
```

```{r}
#| label: q6
# scatter plot relating fish abundance indices to common murre diet.

ggplot(rreas_murres, 
       aes(x = rreas_rockfish_index, 
           y = comu_rockfish_diet)) + 
  geom_point()


```

**Q6: Which variables did you choose for the x- and y-axis? What does this mean for the purpose of your model?**

-   We are looking to the idea of predicting the diet via the rockfish adundance index

### Fit the model

Fit a model that predicts the RREAS rockfish abundance index from the fraction of rockfish in common murre diets.

```{r}
#| eval: false
# Creating a model 
rf_mod <- lm(comu_rockfish_diet ~ rreas_rockfish_index, data = rreas_murres)

# Finding Summary 
rf_sum <- summary(rf_mod)

```

### Interpret the model

Find the $R^2$ value for your model.

**Q7: What does the R2 value tell you about the utility of your model for filling in the survey gap?**

```{r}
#| label: q7

# Finding R Squared 
rf_R_sq <- rf_sum$r.squared # 0.698 

# Explaing the R using a print statement 
print(paste0("The rockfish index explains ", round(rf_R_sq * 100, 3),"% of the varation in the diet"))

print(paste0("Having the rockfish index, reduces the error in the prediction by ", round(rf_R_sq * 100, 3),"%"))

```

Extract the estimated coefficients ($\hat \beta_0, \hat \beta_1$) and standard deviation ($\hat \sigma$) from the model.\
**Q8: In plain language, what do the coefficients and standard deviation of the model represent?**

```{r}
#| label: q8

# Finding Sigma and coeffients 
rf_sigma <- rf_sum$sigma # 0.162 

rf_B0 <- coef(rf_mod)[1] # 0.209 

rf_B1 <- coef(rf_mod)[2] # 0.114

# print statement explaing sigma 
print(paste("simga of", round(rf_sigma, 3), "represents the standard deivation  AKA the SPREAD of the data. This data does not have a large spread."))

# print statement explaining B0 
print(paste("B0 of", round(rf_B0, 3), "is the interecpt. When the index is 0, the diet is", round(rf_B0, 3)))

# print statement explaining B1 
print(paste("B1 of", round(rf_B1, 3), "is the slope When the index increased by 1 unit, the diet increased by", round(rf_B1, 3)))

```

Say it's 2020 and the RREAS has largely been canceled. You ask Point Blue to share their common murre diet data with you and they say it was 32.8% juvenile rockfish. Estimate the predicted value of the juvenile rockfish abundance index.

**Q9: What did you predict the juvenile rockfish abundance index was in 2020?**

```{r}
#| label: q9
#| 
#y = b0 + b1 * x 
# x = (y - b0) / b1 

# The given y 
y <-  0.328 

# solving for x 
x <- (y - rf_B0) / rf_B1

# print statement explaining 
print(paste0("If there was a ", y * 100,  "% juvelile rockfish, there rockfish index is estimated to be ", round(x, 3)))
```

**Q10: Your prediction is the *most likely* outcome, but the actual index won't take that value exactly. How would you estimate an interval that you could confidently say probably contains the actual index?**

```{r}

# 95% of the data is between 2 standard deviations from the mean 
a <- y - (2* rf_sigma)
b <- y + (2* rf_sigma)

print(paste("There is a 95% chance the value will be between", round(a, 1), round(b, 2)))

```

## Recap

In this lab, you fit linear regression models to one predictor and one response variable. You used statistical notation to describe the model, which makes two things unambiguous.

1.  It shows the relationship between the predictor, model coefficients, and the expected value of the response for a given value of the predictor.
2.  It says the observed response values are *normally distributed* around the expected value, with a standard deviation of $\sigma$.

In future weeks, we'll build on this foundation to make more complicated models. You'll add additional predictors, transform parameters to introduce non-linearity, and use non-normal random variables to model other types of responses.
